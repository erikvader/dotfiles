#!/bin/python
import sys
from bs4 import BeautifulSoup  # type: ignore
from typing import (
    Any,
    List,
    Tuple,
    Callable,
    Optional,
    TextIO,
    Iterator,
    TypeVar,
)
from urllib.parse import urlparse, urlunparse, ParseResult, parse_qs
from argparse import ArgumentParser, Namespace
from fcntl import flock, LOCK_EX
from pathlib import Path
from dataclasses import dataclass
from io import SEEK_END
from beautifulsoupUtils import unique_selector  # type: ignore


class ParseException(Exception):
    pass


class UserError(Exception):
    pass


Soup = Any


@dataclass(frozen=True)
class Thing:
    name: str
    key: str
    jsmark: Optional[str] = None


SiteName = str
Scraper = Callable[[Soup, ParseResult], List[Thing]] # TODO: missing kwargs
T = TypeVar("T")
TaggedThing = Tuple[Thing, bool]


class Storage:
    sanitize_table = str.maketrans({"\n": r"\n", "\t": r"\t"})

    def __init__(self, site: SiteName, datadir: Path) -> None:
        self.site = site
        self.datadir = datadir
        self.openFile: Optional[TextIO] = None

    def read(self) -> Iterator[Thing]:
        if self.openFile is None:
            return

        self.openFile.seek(0)
        for l in self.openFile:
            fields = l.rstrip().split("\t")
            if len(fields) != 2:
                raise Exception("wrong number of fields")
            yield Thing(name=fields[0], key=fields[1])

    def append(self, things: Iterator[Thing]) -> None:
        if self.openFile is None:
            raise Exception("Storage is not open")

        self.openFile.seek(0, SEEK_END)
        for t in things:
            self.openFile.write(self._sanitize(t.name))
            self.openFile.write("\t")
            self.openFile.write(self._sanitize(t.key))
            self.openFile.write("\n")

    def overwrite(self, things: Iterator[Thing]) -> None:
        raise NotImplementedError()

    def _sanitize(self, string: str) -> str:
        return string.translate(Storage.sanitize_table)

    def _open(self) -> None:
        if self.openFile is not None:
            raise Exception("can't open twice")
        if not self.datadir.is_dir():
            raise Exception(f"'{self.datadir}' doesn't exist or isn't a directory")

        fullpath = self.datadir / self.site
        self.openFile = open(fullpath, "a+")
        flock(self.openFile, LOCK_EX)

    def _close(self) -> None:
        if self.openFile is not None:
            self.openFile.close()

    def __enter__(self):
        self._open()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self._close()
        return False


def scrape_text(tag, allow_empty=False, recursive=False) -> str:
    ite = tag.descendants if recursive else tag.children
    s = " ".join(c.strip() for c in ite if isinstance(c, str))
    if not allow_empty and not s:
        raise ParseException("tag doesn't have text")
    return s


def extract_path(path: str, expectedLen: int, want: int) -> str:
    assert want >= 0 and want < expectedLen
    parts = path.strip("/").split("/")
    if len(parts) != expectedLen:
        raise ParseException(
            f"unexpected path in url, expected {expectedLen} parts but got {len(parts)}"
        )
    return parts[want]


def change_color_on(tag, color: str) -> str:
    sel = unique_selector(tag)
    if sel is None:
        print("couldn't get selector for {}".format(tag), file=sys.stderr)
        return ""

    return 'document.querySelector("{}").style.color = "{}";\n'.format(sel, color)


def rarbgScraper(soup: Soup, url: ParseResult, **_kwargs) -> List[Thing]:
    def path_to_key(path: str) -> str:
        return extract_path(path, 2, 1)

    def torrentPage():
        title = soup.select_one("td.block h1.black")
        if title is None:
            raise ParseException("couldn't find the name")
        title = scrape_text(title)
        key = path_to_key(url.path)

        return [Thing(name=title, key=key)]

    def searchPage():
        if soup.select_one("table.lista2t") is None:
            raise ParseException("the HTML is not as expected")

        results = []
        for tr in soup.find_all("tr", class_="lista2"):
            tds = list(tr.find_all("td", class_="lista", recursive=False))
            if len(tds) != 8:
                raise ParseException("unexpected amount of td-tags")

            main_a = tds[1].a
            if main_a is None:
                raise ParseException("an a-tag was not found")

            name = scrape_text(main_a)

            if "href" not in main_a.attrs:
                raise ParseException("the a-tag is missing href")
            torrent_url = main_a["href"]
            key = path_to_key(torrent_url)

            results.append(
                Thing(name=name, key=key, jsmark=change_color_on(main_a, "black"))
            )

        return results

    if url.path.startswith("/torrent/"):
        return torrentPage()
    elif url.path.startswith("/torrents.php"):
        return searchPage()
    else:
        raise UserError("wrong page")


def PHScraper(soup: Soup, url: ParseResult, **_kwargs) -> List[Thing]:
    if not url.path.startswith("/view_video.php"):
        raise UserError("wrong page")

    try:
        queries = parse_qs(url.query, strict_parsing=True)
    except ValueError as e:
        raise ParseException("invalid query") from e

    if "viewkey" not in queries:
        raise ParseException("viewkey doesn't exist")

    keys = queries["viewkey"]
    if len(keys) != 1:
        raise ParseException("expected only one viewkey")
    key = keys[0]

    title = soup.select_one("div.title-container > h1.title > span")
    if not title:
        raise ParseException("couldn't find video title")
    name = scrape_text(title)

    return [Thing(name=name, key=key)]


def XVideosScraper(soup: Soup, url: ParseResult, **_kwargs) -> List[Thing]:
    if not url.path.startswith("/video"):
        raise UserError("wrong page")

    key = extract_path(url.path, 2, 0)

    title = soup.select_one("h2.page-title")
    if not title:
        raise ParseException("couldn't find video title")
    name = scrape_text(title)

    return [Thing(name=name, key=key)]


def trettontrettiosjuScraper(soup: Soup, url: ParseResult, **_kwargs) -> List[Thing]:
    def path_to_key(path: str) -> str:
        return extract_path(path, 3, 1)

    def torrentPage():
        title = soup.select_one("div.box-info-heading > h1")
        if title is None:
            raise ParseException("couldn't find the name")
        title = scrape_text(title)
        if not title:
            raise ParseException("title name empty")
        key = path_to_key(url.path)

        return [Thing(name=title, key=key)]

    def searchPage():
        if soup.select_one("table.table-list") is None:
            raise ParseException("the HTML is not as expected")

        results = []
        for td in soup.select("table.table-list > tbody > tr > td.name"):
            ajs = list(td.select("a"))
            if len(ajs) != 2:
                raise ParseException("unexpected amount of a-tags")
            main_a = ajs[1]

            name = scrape_text(main_a)
            if not name:
                raise ParseException("title name empty")

            if "href" not in main_a.attrs:
                raise ParseException("the a-tag is missing href")
            key = path_to_key(main_a["href"])

            results.append(Thing(name=name, key=key))

        return results

    if url.path.startswith("/torrent/"):
        return torrentPage()
    elif url.path.startswith("/search/"):
        return searchPage()
    else:
        raise UserError("wrong page")


def TnaFlixScraper(soup: Soup, url: ParseResult, **_kwargs) -> List[Thing]:
    try:
        key = extract_path(url.path, 3, 2)
        if not key.startswith("video"):
            raise UserError("wrong page")
    except ParseException as e:
        raise UserError("wrong page") from e

    title = soup.select_one("div.sectHeader > h1")
    if not title:
        raise ParseException("couldn't find video title")
    name = scrape_text(title)
    if not name:
        raise ParseException("title name empty")

    return [Thing(name=name, key=key)]


def SpankbangScraper(
    soup: Soup,
    url: ParseResult,
    everything: bool,
    **_kwargs,
) -> List[Thing]:
    def video_key(urlpath: str) -> Optional[str]:
        try:
            test = extract_path(urlpath, 3, 1)
            if test != "video":
                return None
        except ParseException:
            return None
        return extract_path(urlpath, 3, 0)

    def current_page() -> List[Thing]:
        if (key := video_key(url.path)) is None:
            raise UserError("wrong page")

        title = soup.select_one("#video > div.left > h1[title]")
        if not title:
            raise ParseException("couldn't find video title")
        name = title["title"]
        if not name:
            raise ParseException("title name empty")

        return [Thing(name=name, key=key, jsmark=change_color_on(title, "lime"))]

    def findall() -> List[Thing]:
        if video_key(url.path) is not None:
            things = current_page()
        else:
            things = []

        others = soup.select("div.video-list > div.video-item > a.n")
        for video in others:
            if "href" not in video.attrs:
                continue
            href = video["href"]
            if (key := video_key(href)) is None:
                continue
            name = scrape_text(video, recursive=True)
            things.append(
                Thing(name=name, key=key, jsmark=change_color_on(video, "lime"))
            )
        return things

    return findall() if everything else current_page()


def XHamsterScraper(
    soup: Soup,
    url: ParseResult,
    everything: bool = False,
    **_kwargs,
) -> List[Thing]:
    def video_key(urlpath: str) -> Optional[str]:
        try:
            test = extract_path(urlpath, 2, 0)
            if test != "videos":
                return None
        except ParseException:
            return None
        return extract_path(urlpath, 2, 1)

    def current_page() -> List[Thing]:
        if (key := video_key(url.path)) is None:
            raise UserError("wrong page")

        title = soup.select_one("main > div.with-player-container > h1:first-child")
        if not title:
            raise ParseException("couldn't find video title")
        name = scrape_text(title)
        if not name:
            raise ParseException("title name empty")

        return [Thing(name=name, key=key, jsmark=change_color_on(title, "lime"))]

    def findall() -> List[Thing]:
        if video_key(url.path) is not None:
            things = current_page()
        else:
            things = []

        others = soup.select(
            "div.thumb-list > div.video-thumb > div.video-thumb-info > a.video-thumb-info__name"
        )
        for video in others:
            if "href" not in video.attrs:
                continue
            href = urlparse(video["href"]).path
            if (key := video_key(href)) is None:
                continue
            name = scrape_text(video)
            things.append(
                Thing(name=name, key=key, jsmark=change_color_on(video, "lime"))
            )
        return things

    return findall() if everything else current_page()


def XnxxScraper(soup: Soup, url: ParseResult, **_kwargs) -> List[Thing]:
    if not url.path.startswith("/video-"):
        raise UserError("wrong page")

    key = extract_path(url.path, 2, 0)
    title = soup.select_one("p.video-title")
    if not title:
        raise ParseException("couldn't find video title")
    name = scrape_text(title)
    if not name:
        raise ParseException("title name empty")

    return [Thing(name=name, key=key)]


def theyAreHugeScraper(soup: Soup, url: ParseResult, **_kwargs) -> List[Thing]:
    if not url.path.startswith("/v/"):
        raise UserError("wrong page")

    key = extract_path(url.path, 2, 1)
    title = soup.select_one("h1.cs_headline")
    if not title:
        raise ParseException("couldn't find video title")
    name = scrape_text(title)
    if not name:
        raise ParseException("title name empty")

    return [Thing(name=name, key=key)]


def myhentaigalleryScraper(
    soup: Soup,
    url: ParseResult,
    everything: bool = False,
    **_kwargs,
) -> List[Thing]:
    def comic_key(urlpath):
        if not urlpath.startswith("/gallery/thumbnails/"):
            return None
        try:
            return extract_path(urlpath, 3, 2)
        except ParseException:
            return None

    def current_page():
        if (key := comic_key(url.path)) is None:
            raise UserError("wrong page")

        title = soup.select_one("div.comic-description > h1")
        if not title:
            raise ParseException("couldn't find video title")
        name = scrape_text(title)
        if not name:
            raise ParseException("title name empty")

        return [Thing(name=name, key=key, jsmark=change_color_on(title, "lime"))]

    def findall():
        if comic_key(url.path) is not None:
            return current_page()

        things = []
        others = soup.select(
            "div.comic-listing > ul.comics-grid > li.item > div.comic-inner > a"
        )
        for comic in others:
            if "href" not in comic.attrs:
                continue
            href = comic["href"]
            if (key := comic_key(href)) is None:
                continue
            nametag = comic.select_one("div.comic-info > h2.comic-name")
            if nametag is None:
                raise ParseException("comic did not have a comic-name")
            name = scrape_text(nametag)
            things.append(
                Thing(name=name, key=key, jsmark=change_color_on(nametag, "lime"))
            )
        return things

    return findall() if everything else current_page()


def get_scraper(hostname: SiteName) -> Optional[Scraper]:
    return {
        "rarbg": rarbgScraper,
        "pornhub": PHScraper,
        "xvideos": XVideosScraper,
        "1337x": trettontrettiosjuScraper,
        "tnaflix": TnaFlixScraper,
        "spankbang": SpankbangScraper,
        "xnxx": XnxxScraper,
        "theyarehuge": theyAreHugeScraper,
        "myhentaigallery": myhentaigalleryScraper,
        "xhamster": XHamsterScraper,
    }.get(hostname)


def url_to_site_name(url: ParseResult) -> Optional[SiteName]:
    if not url.hostname:
        return None

    parts: List[str] = url.hostname.lower().split(".")
    if not parts:
        return None
    elif len(parts) <= 2:
        # http://sitename
        # http://sitename.tld
        return parts[0]
    else:
        # http://asd.basd.qwe.sitename.tld
        return parts[-2]


def parse_arguments() -> Namespace:
    parser = ArgumentParser(
        prog="rememberer",
        description="""
        Give the url of a webstite as an argument and it's HTML to stdin and I will
        remember that you have been there.
        """,
    )
    g = parser.add_mutually_exclusive_group(required=True)
    g.add_argument(
        "url", type=urlparse, nargs="?", help="The full URL the page is from"
    )
    g.add_argument(
        "--replay",
        action="store_true",
        help="Show the previous output again",
    )

    parser.add_argument(
        "--no-remember",
        action="store_true",
        help="Don't remember new things",
    )

    parser.add_argument(
        "--everything",
        action="store_true",
        help="find more on the page, not the current",
    )

    parser.add_argument(
        "--test",
        action="store_true",
        help="""
        Set exit status to 0 if there were new things added, 1 if everything is old and 2
        if an error occured
        """,
    )

    parser.add_argument(
        "--quiet", action="store_true", help="Don't print what is new and old"
    )

    return parser.parse_args()


def create_data_dir() -> Path:
    datadir = Path.home() / ".local" / "share" / "rememberer"
    datadir.mkdir(parents=True, exist_ok=True)
    return datadir


def group(lista: Iterator[T], key: Callable[[T], Any] = lambda x: x) -> List[List[T]]:
    res = []
    cur: List[T] = []
    for i in lista:
        if not cur or key(cur[0]) == key(i):
            cur.append(i)
        else:
            res.append(cur)
            cur = [i]

    if cur:
        res.append(cur)
    return res


def print_new_things(
    things: List[TaggedThing],
    printer: Callable[[str], None] = print,
    inverse: bool = False,
) -> None:
    def age_word(say_new: bool) -> str:
        return "new" if say_new else "old"

    def print_all(print_new: bool) -> None:
        new = [elem for elem, isnew in things if isnew == print_new]
        printer("all of the following are " + age_word(print_new))
        for i in new:
            printer(i.name)

    num = len(things)

    if num == 0:
        printer("found nothing")
        return
    elif num == 1:
        if not inverse:
            printer(f"this is {age_word(things[0][1])}")
        else:
            printer("no inverse")
        return

    printer("found a total of {} things".format(num))

    num_new = sum(1 for t, b in things if b)
    only_one_new = num_new == 1
    only_one_old = num - num_new == 1
    if only_one_new or only_one_old:
        target = True if only_one_new else False

        if inverse:
            print_all(not target)
            return

        for t, b in things:
            if b != target:
                continue
            printer(
                "only {} is {}".format(
                    t.name,
                    age_word(target),
                )
            )
            return

    groups = group(iter(things), lambda x: x[1])

    if len(groups) == 1:
        if not inverse:
            printer(f"everything here is {age_word(groups[0][0][1])}")
        else:
            printer("no inverse")
        return

    if len(groups) == 2:
        upper = groups[0]
        lower = groups[1]

        if (len(upper) <= len(lower)) ^ inverse:
            critical = upper[-1]
            where = "above"
        else:
            critical = lower[0]
            where = "below"

        printer(
            "everything {} the following is {}:".format(where, age_word(critical[1]))
        )
        printer(critical[0].name)
        return

    if len(groups) == 3 and len(middle := groups[1]) >= 2:
        if not inverse:
            printer(
                "everything between\n{}\nand\n{}\nis {}".format(
                    middle[0][0].name, middle[-1][0].name, age_word(middle[0][1])
                )
            )
        else:
            print_all(not middle[0][1])
        return

    print_all(True ^ inverse)


def replay(lastoutputfile: Path, prefix: str = "Replay of: ") -> None:
    import shutil

    try:
        with open(lastoutputfile, "r") as f:
            print(prefix, end="")
            shutil.copyfileobj(f, sys.stdout)
    except FileNotFoundError:
        print("Nothing to replay")


def write_mark_js(jsfile: Path, things: Iterator[TaggedThing]):
    with open(jsfile, "w") as f:
        for t in (t for t, is_new in things if not is_new):
            if t.jsmark is not None:
                f.write(t.jsmark)


def write_results(
    url: ParseResult,
    things: List[TaggedThing],
    save_file: Path,
    to_stdout=True,
) -> None:
    with open(save_file, "a+") as f:
        flock(f, LOCK_EX)
        f.seek(0)
        f.truncate(0)

        def tee(line: str) -> None:
            if to_stdout:
                print(line)
            print(line, file=f)

        print(urlunparse(url), file=f)
        print_new_things(things, printer=tee)


def remember(
    url: ParseResult,
    datadir: Path,
    lastoutputfile: Path,
    lastjsfile: Path,
    quiet: bool = False,
    save: bool = True,
    everything: bool = False,
) -> bool:
    if not url.hostname or not url.scheme:
        raise UserError("url is not fully qualified or is invalid")
    if url.scheme not in ["http", "https"]:
        raise UserError("url must be http or https")

    # TODO: fallback to html5lib somehow if lxml fails? Good if the website is pretty broken
    webpage = BeautifulSoup(sys.stdin, "lxml")

    siteName = url_to_site_name(url)
    if not siteName:
        raise UserError(f"couldn't extract a site name from {url.hostname}")

    scraper = get_scraper(siteName)
    if scraper is None:
        raise UserError("'{}' is not supported".format(url.hostname))

    things = scraper(webpage, url, everything=everything)

    if not things:
        raise Exception("didn't find any things but the scraper didn't error")

    with Storage(siteName, datadir) as s:
        keys = set(t.key for t in s.read())
        new_things = [t.key not in keys for t in things]
        tagged = list(zip(things, new_things))
        if save:
            s.append(t for t, is_new in tagged if is_new)

    write_results(url, tagged, lastoutputfile, to_stdout=not quiet)
    write_mark_js(lastjsfile, iter(tagged))

    return any(new_things)


def run(args: Namespace) -> bool:
    datadir = create_data_dir()
    lastoutputfile = datadir / ".last_output"
    lastjsfile = datadir / ".last_js"

    if args.replay:
        replay(lastoutputfile)
        return True
    elif args.url:
        return remember(
            args.url,
            datadir,
            lastoutputfile,
            lastjsfile,
            args.quiet,
            not args.no_remember,
            args.everything,
        )
    else:
        raise Exception("one of these options should have been given")

    return False


def main():
    args = parse_arguments()

    try:
        success = run(args)
        if args.test:
            exitcode = 0 if success else 1
        else:
            exitcode = 0
        exit(exitcode)
    except UserError as ue:
        print(ue, end="", file=sys.stderr)
        exit(2)
    except Exception:  # pylint: disable=broad-except
        import traceback

        traceback.print_exc()
        exit(2)


if __name__ == "__main__":
    main()
